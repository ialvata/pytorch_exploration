{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchmetrics import F1Score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import ipaddress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Botnet Detection\n",
    "## A Multiclass Classification Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset can be found [zenodo.org/records/8035724](https://zenodo.org/records/8035724)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the research paper [An empirical comparison of botnet detection methods](https://doi.org/10.1016/j.cose.2014.05.011) we have some information on the features used.\n",
    "1. Source IP address\n",
    "2. Amount of unique source ports used by this source IP address.\n",
    "3. Amount of unique destination IP addresses contacted by this source IP address.\n",
    "4. Amount of unique destination ports contacted by this source IP address.\n",
    "5. Amount of NetFlows used by this source IP address.\n",
    "6. Amount of bytes transferred by this source IP address.\n",
    "7. Amount of packets transferred by this source IP address.\n",
    "\n",
    "After clustering some of this observations, the authors created some more features:\n",
    "1. Total amount of instances in the cluster.\n",
    "2. Total amount of NetFlows in the cluster.\n",
    "3. Amount of source IP addresses.\n",
    "4. Average amount of unique source ports.\n",
    "5. Standard Deviation of the amount of unique source ports.\n",
    "6. Average amount of unique destination IP addresses.\n",
    "7. Standard Deviation of the amount of unique destination IP addresses.\n",
    "8. Average amount of unique destination ports.\n",
    "9. Standard Deviation of the amount of unique destination ports.\n",
    "10. Average amount of NetFlows.\n",
    "11. Standard Deviation of the amount of NetFlows.\n",
    "12. Average amount of bytes transferred.\n",
    "13. Standard Deviation of the amount of bytes transferred.\n",
    "14. Average amount of packets transferred.\n",
    "15. Standard Deviation of the amount of packets transferred.\n",
    "\n",
    "The remaining features should be self-explanatory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"./data/botnet_multiclass.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 176640 entries, 0 to 176639\n",
      "Columns: 85 entries, Unnamed: 0.1 to LABEL\n",
      "dtypes: bool(4), float64(61), int64(17), object(3)\n",
      "memory usage: 109.8+ MB\n"
     ]
    }
   ],
   "source": [
    "raw_data.info(verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(raw_data.isna()).any().any() # True means that we have missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to transform the object columns, and deal with missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data = raw_data.copy(deep=True) \n",
    "# since we have enough memory, it should be ok. This will avoid messing the original data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['DST_IP', 'SRC_IP', 'LABEL'], dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object_columns = preprocessed_data.select_dtypes(include=['object'])\n",
    "object_columns.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's encode the destination and source IP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DST_IP</th>\n",
       "      <th>SRC_IP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>147.32.80.9</td>\n",
       "      <td>147.32.84.165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>147.32.84.255</td>\n",
       "      <td>147.32.84.165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>147.32.84.255</td>\n",
       "      <td>147.32.84.165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>147.32.84.165.</td>\n",
       "      <td>60.190.222.139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60.190.222.139.</td>\n",
       "      <td>147.32.84.165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            DST_IP          SRC_IP\n",
       "0      147.32.80.9   147.32.84.165\n",
       "1    147.32.84.255   147.32.84.165\n",
       "2    147.32.84.255   147.32.84.165\n",
       "3   147.32.84.165.  60.190.222.139\n",
       "4  60.190.222.139.   147.32.84.165"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object_columns[['DST_IP', 'SRC_IP']].head()\n",
    "# notice row with index 3. There's an extra final \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ip_encoding(ip_address:str) -> int:\n",
    "    return int(ipaddress.ip_address(ip_address.rstrip(\".\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IPv4Address('147.32.84.165')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipaddress.ip_address(ip_encoding(\"147.32.84.165.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DST_IP</th>\n",
       "      <th>SRC_IP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2468368393</td>\n",
       "      <td>2468369573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2468369663</td>\n",
       "      <td>2468369573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2468369663</td>\n",
       "      <td>2468369573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2468369573</td>\n",
       "      <td>1019141771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1019141771</td>\n",
       "      <td>2468369573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       DST_IP      SRC_IP\n",
       "0  2468368393  2468369573\n",
       "1  2468369663  2468369573\n",
       "2  2468369663  2468369573\n",
       "3  2468369573  1019141771\n",
       "4  1019141771  2468369573"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data['DST_IP'] = preprocessed_data['DST_IP'].apply(ip_encoding)\n",
    "preprocessed_data[\"SRC_IP\"] = preprocessed_data['SRC_IP'].apply(ip_encoding)\n",
    "preprocessed_data[['DST_IP',\"SRC_IP\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_enc = LabelEncoder()\n",
    "preprocessed_data[\"LABEL\"] = label_enc.fit_transform(preprocessed_data[\"LABEL\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['clear', 'donbot', 'fast_flux', 'neris', 'qvod', 'rbot'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_enc.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The zero class represents 'normalware', whereas all the remaining classes represent some sort of malware, more specifically botnets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a small EDA on the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LABEL\n",
       "0        165573\n",
       "3          6332\n",
       "2          4367\n",
       "4           286\n",
       "5            55\n",
       "1            27\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_counts = preprocessed_data[[\"LABEL\"]].value_counts()\n",
    "labels_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels_counts.plot.pie(); # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset is highly imbalanced, to the point that classifying some classes could be considered as outlier detection. There will be a section focused on this problem later on.\n",
    "\n",
    "For now, we will conduct our analysis without worrying to much on the class imbalance problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150598, 85)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data[preprocessed_data.isna().any(axis=1)].shape\n",
    "# there are 150598 row whose values have at least an NaN, leaving us with \n",
    "# only 26042 rows which are completely filled. Even though torch NN can take NaNs values, \n",
    "# these will contaminate the calculations, resulting in predictions full of NaNs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep this notebook simple, I'll just drop the rows with NaN's values.\n",
    "In a more demanding setting, we should decide on an imputation method for each feature, *separately*. And depending on the chosen method, we may only apply it after we've splitted the data into train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 26042 entries, 1 to 176607\n",
      "Columns: 85 entries, Unnamed: 0.1 to LABEL\n",
      "dtypes: bool(4), float64(61), int64(20)\n",
      "memory usage: 16.4 MB\n"
     ]
    }
   ],
   "source": [
    "preprocessed_data = preprocessed_data.dropna()\n",
    "preprocessed_data.info(verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3571, 241]),\n",
       " torch.Size([3571, 1]),\n",
       " torch.Size([893, 241]),\n",
       " torch.Size([893, 1]))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separating features and label\n",
    "X = torch.tensor(preprocessed_data.iloc[:,:-1].values, dtype=torch.float16)\n",
    "# float16 uses less memory\n",
    "y = torch.tensor(preprocessed_data[[\"Label\"]].values, dtype=torch.float16)\n",
    "# Create train/test split\n",
    "train_split = int(0.8 * len(X)) # 80% of data used for training set, 20% for testing \n",
    "X_train, y_train = X[:train_split], y[:train_split]\n",
    "\n",
    "X_test, y_test = X[train_split:], y[train_split:]\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make device agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Construct a model class that subclasses nn.Module\n",
    "class BotNetClassifier(nn.Module):\n",
    "    def __init__(self, bce_loss_with_logits:bool = True):\n",
    "        super().__init__()\n",
    "        self.train_loss_values = []\n",
    "        self.test_loss_values = []\n",
    "        self.bce_loss_with_logits = bce_loss_with_logits\n",
    "        self.f1 = F1Score(task=\"binary\", num_classes=2)\n",
    "        # 2. Create 2 nn.Linear layers capable of handling X and y input and output shapes\n",
    "        self.layer_1 = nn.Linear(in_features=241, out_features=5, dtype=torch.float16) \n",
    "        # takes in 241 features (X), produces 5 features\n",
    "        # since the matrix is very sparse, I wonder if we can compress the information\n",
    "        self.layer_2 = nn.Linear(in_features=5, out_features=1, dtype=torch.float16)\n",
    "            # produces 1 feature, since we're in a binary classification problem\n",
    "        if not bce_loss_with_logits:\n",
    "            self.layer_3 = nn.Sigmoid(in_features=5, out_features=1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return  self.layer_2(self.layer_1(x))\n",
    "    \n",
    "    def train_loop(self, input: torch.Tensor, labels: torch.Tensor, \n",
    "                   loss_fn:nn.modules.loss._Loss, optimizer: torch.optim.Optimizer):\n",
    "        self.train()\n",
    "        # Forward pass on train data using the forward() method\n",
    "        if self.bce_loss_with_logits:\n",
    "            # bce_loss_with_logits works directly with logits\n",
    "            pred = self(input)\n",
    "        else:\n",
    "            pred = torch.round(self(input)) \n",
    "        loss = loss_fn(pred, labels)\n",
    "        f1_value = self.f1(pred, labels)\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        # optimize parameters\n",
    "        optimizer.step()\n",
    "        # restart cycle\n",
    "        optimizer.zero_grad()\n",
    "        return loss, f1_value\n",
    "    \n",
    "    def test_loop(self, input: torch.Tensor, labels: torch.Tensor, \n",
    "                  loss_fn:nn.modules.loss._Loss):\n",
    "        self.eval()\n",
    "        with torch.inference_mode():\n",
    "            pred = self(input)\n",
    "            loss = loss_fn(pred, labels)\n",
    "            f1_value = self.f1(pred, labels)\n",
    "        return loss, f1_value\n",
    "    \n",
    "    def train_model(self, input: torch.Tensor, labels: torch.Tensor,\n",
    "                    loss_fn:nn.modules.loss._Loss, optimizer: torch.optim.Optimizer,\n",
    "                    n_epochs:int = 100):\n",
    "        for epoch in range(1,n_epochs+1):\n",
    "            print(f\"Epoch {epoch}\\n-------------------------------\")\n",
    "            train_loss, train_f1 = self.train_loop(input = input, labels = labels, \n",
    "                                         loss_fn = loss_fn, optimizer = optimizer)\n",
    "            self.train_loss_values.append(train_loss.detach().cpu().numpy())\n",
    "            test_loss, test_f1 = self.test_loop(input = input, labels = labels, \n",
    "                                       loss_fn = loss_fn)\n",
    "            self.test_loss_values.append(test_loss.detach().cpu().numpy())\n",
    "            print(\n",
    "                f\"{loss_fn._get_name()} Train Loss: {train_loss} |\"\n",
    "                f\"{loss_fn._get_name()} Test Loss: {test_loss}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"F1 Train Score: {train_f1}  | F1 Test Score: {test_f1}\"\n",
    "            )\n",
    "        print(\"Done!\")\n",
    "    \n",
    "    def predict(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        self.eval()\n",
    "        with torch.inference_mode():\n",
    "            pred = self(input)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MalwareLinearClassifier(\n",
       "  (f1): BinaryF1Score()\n",
       "  (layer_1): Linear(in_features=241, out_features=5, bias=True)\n",
       "  (layer_2): Linear(in_features=5, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Create an instance of the model and send it to target device\n",
    "linear_cl = BotNetClassifier().to(device)\n",
    "linear_cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2734],\n",
       "        [-0.2812],\n",
       "        [-0.2986],\n",
       "        ...,\n",
       "        [-0.3066],\n",
       "        [-0.3188],\n",
       "        [-0.3511]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_cl(X_train.to(device)) # simple litmus test to check whether it's working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a loss function\n",
    "# loss_fn = nn.BCELoss() # BCELoss = no sigmoid built-in\n",
    "loss_fn = nn.BCEWithLogitsLoss() # BCEWithLogitsLoss = sigmoid built-in\n",
    "\n",
    "# Create an optimizer\n",
    "optimizer = torch.optim.SGD(params=linear_cl.parameters(), \n",
    "                            lr=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.8291015625 |BCEWithLogitsLoss Test Loss: 0.71533203125\n",
      "F1 Train Score: 0.0  | F1 Test Score: 0.3769735097885132\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.71533203125 |BCEWithLogitsLoss Test Loss: 0.62548828125\n",
      "F1 Train Score: 0.3769735097885132  | F1 Test Score: 0.9489138722419739\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.62548828125 |BCEWithLogitsLoss Test Loss: 0.552734375\n",
      "F1 Train Score: 0.9489138722419739  | F1 Test Score: 0.9493876099586487\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.552734375 |BCEWithLogitsLoss Test Loss: 0.49365234375\n",
      "F1 Train Score: 0.9493876099586487  | F1 Test Score: 0.9497058987617493\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.49365234375 |BCEWithLogitsLoss Test Loss: 0.4462890625\n",
      "F1 Train Score: 0.9497058987617493  | F1 Test Score: 0.9497058987617493\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.4462890625 |BCEWithLogitsLoss Test Loss: 0.408447265625\n",
      "F1 Train Score: 0.9497058987617493  | F1 Test Score: 0.9498603343963623\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.408447265625 |BCEWithLogitsLoss Test Loss: 0.378173828125\n",
      "F1 Train Score: 0.9498603343963623  | F1 Test Score: 0.954166054725647\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.378173828125 |BCEWithLogitsLoss Test Loss: 0.353759765625\n",
      "F1 Train Score: 0.954166054725647  | F1 Test Score: 0.954166054725647\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.353759765625 |BCEWithLogitsLoss Test Loss: 0.334228515625\n",
      "F1 Train Score: 0.954166054725647  | F1 Test Score: 0.954166054725647\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.334228515625 |BCEWithLogitsLoss Test Loss: 0.318115234375\n",
      "F1 Train Score: 0.954166054725647  | F1 Test Score: 0.954166054725647\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.318115234375 |BCEWithLogitsLoss Test Loss: 0.3046875\n",
      "F1 Train Score: 0.954166054725647  | F1 Test Score: 0.954166054725647\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.3046875 |BCEWithLogitsLoss Test Loss: 0.293212890625\n",
      "F1 Train Score: 0.954166054725647  | F1 Test Score: 0.954166054725647\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.293212890625 |BCEWithLogitsLoss Test Loss: 0.28369140625\n",
      "F1 Train Score: 0.954166054725647  | F1 Test Score: 0.954166054725647\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.28369140625 |BCEWithLogitsLoss Test Loss: 0.27490234375\n",
      "F1 Train Score: 0.954166054725647  | F1 Test Score: 0.954166054725647\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.27490234375 |BCEWithLogitsLoss Test Loss: 0.26708984375\n",
      "F1 Train Score: 0.954166054725647  | F1 Test Score: 0.954166054725647\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.26708984375 |BCEWithLogitsLoss Test Loss: 0.26025390625\n",
      "F1 Train Score: 0.954166054725647  | F1 Test Score: 0.954166054725647\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.26025390625 |BCEWithLogitsLoss Test Loss: 0.253662109375\n",
      "F1 Train Score: 0.954166054725647  | F1 Test Score: 0.954166054725647\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.253662109375 |BCEWithLogitsLoss Test Loss: 0.24755859375\n",
      "F1 Train Score: 0.954166054725647  | F1 Test Score: 0.954166054725647\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.24755859375 |BCEWithLogitsLoss Test Loss: 0.24169921875\n",
      "F1 Train Score: 0.954166054725647  | F1 Test Score: 0.954166054725647\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.24169921875 |BCEWithLogitsLoss Test Loss: 0.236083984375\n",
      "F1 Train Score: 0.954166054725647  | F1 Test Score: 0.954166054725647\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.236083984375 |BCEWithLogitsLoss Test Loss: 0.230712890625\n",
      "F1 Train Score: 0.954166054725647  | F1 Test Score: 0.954166054725647\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.230712890625 |BCEWithLogitsLoss Test Loss: 0.2257080078125\n",
      "F1 Train Score: 0.954166054725647  | F1 Test Score: 0.954166054725647\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.2257080078125 |BCEWithLogitsLoss Test Loss: 0.220703125\n",
      "F1 Train Score: 0.954166054725647  | F1 Test Score: 0.954166054725647\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.220703125 |BCEWithLogitsLoss Test Loss: 0.2158203125\n",
      "F1 Train Score: 0.954166054725647  | F1 Test Score: 0.9544456005096436\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.2158203125 |BCEWithLogitsLoss Test Loss: 0.2110595703125\n",
      "F1 Train Score: 0.9544456005096436  | F1 Test Score: 0.9544456005096436\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.2110595703125 |BCEWithLogitsLoss Test Loss: 0.2064208984375\n",
      "F1 Train Score: 0.9544456005096436  | F1 Test Score: 0.9547252655029297\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.2064208984375 |BCEWithLogitsLoss Test Loss: 0.20166015625\n",
      "F1 Train Score: 0.9547252655029297  | F1 Test Score: 0.9547252655029297\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.20166015625 |BCEWithLogitsLoss Test Loss: 0.197265625\n",
      "F1 Train Score: 0.9547252655029297  | F1 Test Score: 0.9551451206207275\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.197265625 |BCEWithLogitsLoss Test Loss: 0.19287109375\n",
      "F1 Train Score: 0.9551451206207275  | F1 Test Score: 0.9554252028465271\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.19287109375 |BCEWithLogitsLoss Test Loss: 0.1884765625\n",
      "F1 Train Score: 0.9554252028465271  | F1 Test Score: 0.955565333366394\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.1884765625 |BCEWithLogitsLoss Test Loss: 0.184326171875\n",
      "F1 Train Score: 0.955565333366394  | F1 Test Score: 0.9564068913459778\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.184326171875 |BCEWithLogitsLoss Test Loss: 0.1800537109375\n",
      "F1 Train Score: 0.9564068913459778  | F1 Test Score: 0.9579535722732544\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.1800537109375 |BCEWithLogitsLoss Test Loss: 0.176025390625\n",
      "F1 Train Score: 0.9579535722732544  | F1 Test Score: 0.9586582183837891\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.176025390625 |BCEWithLogitsLoss Test Loss: 0.1719970703125\n",
      "F1 Train Score: 0.9586582183837891  | F1 Test Score: 0.959787905216217\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.1719970703125 |BCEWithLogitsLoss Test Loss: 0.1680908203125\n",
      "F1 Train Score: 0.959787905216217  | F1 Test Score: 0.9614874124526978\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.1680908203125 |BCEWithLogitsLoss Test Loss: 0.1641845703125\n",
      "F1 Train Score: 0.9614874124526978  | F1 Test Score: 0.9619131684303284\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.1641845703125 |BCEWithLogitsLoss Test Loss: 0.1602783203125\n",
      "F1 Train Score: 0.9619131684303284  | F1 Test Score: 0.9637627601623535\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.1602783203125 |BCEWithLogitsLoss Test Loss: 0.1566162109375\n",
      "F1 Train Score: 0.9637627601623535  | F1 Test Score: 0.9656194448471069\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.1566162109375 |BCEWithLogitsLoss Test Loss: 0.15283203125\n",
      "F1 Train Score: 0.9656194448471069  | F1 Test Score: 0.9663354754447937\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.15283203125 |BCEWithLogitsLoss Test Loss: 0.1495361328125\n",
      "F1 Train Score: 0.9663354754447937  | F1 Test Score: 0.9666221737861633\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.1495361328125 |BCEWithLogitsLoss Test Loss: 0.1461181640625\n",
      "F1 Train Score: 0.9666221737861633  | F1 Test Score: 0.9676269888877869\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.1461181640625 |BCEWithLogitsLoss Test Loss: 0.1427001953125\n",
      "F1 Train Score: 0.9676269888877869  | F1 Test Score: 0.9684898853302002\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.1427001953125 |BCEWithLogitsLoss Test Loss: 0.139404296875\n",
      "F1 Train Score: 0.9684898853302002  | F1 Test Score: 0.9690660238265991\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.139404296875 |BCEWithLogitsLoss Test Loss: 0.136474609375\n",
      "F1 Train Score: 0.9690660238265991  | F1 Test Score: 0.9700759053230286\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.136474609375 |BCEWithLogitsLoss Test Loss: 0.13330078125\n",
      "F1 Train Score: 0.9700759053230286  | F1 Test Score: 0.9707985520362854\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.13330078125 |BCEWithLogitsLoss Test Loss: 0.1302490234375\n",
      "F1 Train Score: 0.9707985520362854  | F1 Test Score: 0.9716671705245972\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.1302490234375 |BCEWithLogitsLoss Test Loss: 0.12744140625\n",
      "F1 Train Score: 0.9716671705245972  | F1 Test Score: 0.9735544323921204\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.12744140625 |BCEWithLogitsLoss Test Loss: 0.12469482421875\n",
      "F1 Train Score: 0.9735544323921204  | F1 Test Score: 0.9747195243835449\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.12469482421875 |BCEWithLogitsLoss Test Loss: 0.12188720703125\n",
      "F1 Train Score: 0.9747195243835449  | F1 Test Score: 0.9760335683822632\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.12188720703125 |BCEWithLogitsLoss Test Loss: 0.119384765625\n",
      "F1 Train Score: 0.9760335683822632  | F1 Test Score: 0.9766187071800232\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.119384765625 |BCEWithLogitsLoss Test Loss: 0.1165771484375\n",
      "F1 Train Score: 0.9766187071800232  | F1 Test Score: 0.9777911305427551\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.1165771484375 |BCEWithLogitsLoss Test Loss: 0.1142578125\n",
      "F1 Train Score: 0.9777911305427551  | F1 Test Score: 0.9789663553237915\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.1142578125 |BCEWithLogitsLoss Test Loss: 0.1119384765625\n",
      "F1 Train Score: 0.9789663553237915  | F1 Test Score: 0.9795550107955933\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.1119384765625 |BCEWithLogitsLoss Test Loss: 0.1094970703125\n",
      "F1 Train Score: 0.9795550107955933  | F1 Test Score: 0.9802918434143066\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.1094970703125 |BCEWithLogitsLoss Test Loss: 0.10736083984375\n",
      "F1 Train Score: 0.9802918434143066  | F1 Test Score: 0.9825090765953064\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.10736083984375 |BCEWithLogitsLoss Test Loss: 0.10528564453125\n",
      "F1 Train Score: 0.9825090765953064  | F1 Test Score: 0.9825090765953064\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.10528564453125 |BCEWithLogitsLoss Test Loss: 0.10321044921875\n",
      "F1 Train Score: 0.9825090765953064  | F1 Test Score: 0.9833987355232239\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.10321044921875 |BCEWithLogitsLoss Test Loss: 0.10125732421875\n",
      "F1 Train Score: 0.9833987355232239  | F1 Test Score: 0.9841413497924805\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.10125732421875 |BCEWithLogitsLoss Test Loss: 0.0989990234375\n",
      "F1 Train Score: 0.9841413497924805  | F1 Test Score: 0.9845874905586243\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.0989990234375 |BCEWithLogitsLoss Test Loss: 0.09710693359375\n",
      "F1 Train Score: 0.9845874905586243  | F1 Test Score: 0.9845874905586243\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.09710693359375 |BCEWithLogitsLoss Test Loss: 0.09527587890625\n",
      "F1 Train Score: 0.9845874905586243  | F1 Test Score: 0.9853318929672241\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.09527587890625 |BCEWithLogitsLoss Test Loss: 0.09356689453125\n",
      "F1 Train Score: 0.9853318929672241  | F1 Test Score: 0.9857791066169739\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.09356689453125 |BCEWithLogitsLoss Test Loss: 0.09197998046875\n",
      "F1 Train Score: 0.9857791066169739  | F1 Test Score: 0.9863760471343994\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.09197998046875 |BCEWithLogitsLoss Test Loss: 0.09033203125\n",
      "F1 Train Score: 0.9863760471343994  | F1 Test Score: 0.9865253567695618\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.09033203125 |BCEWithLogitsLoss Test Loss: 0.0887451171875\n",
      "F1 Train Score: 0.9865253567695618  | F1 Test Score: 0.9862183928489685\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.0887451171875 |BCEWithLogitsLoss Test Loss: 0.087158203125\n",
      "F1 Train Score: 0.9862183928489685  | F1 Test Score: 0.987261176109314\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.087158203125 |BCEWithLogitsLoss Test Loss: 0.0855712890625\n",
      "F1 Train Score: 0.987261176109314  | F1 Test Score: 0.9880103468894958\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.0855712890625 |BCEWithLogitsLoss Test Loss: 0.08416748046875\n",
      "F1 Train Score: 0.9880103468894958  | F1 Test Score: 0.9883103370666504\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.08416748046875 |BCEWithLogitsLoss Test Loss: 0.08282470703125\n",
      "F1 Train Score: 0.9883103370666504  | F1 Test Score: 0.9886105060577393\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.08282470703125 |BCEWithLogitsLoss Test Loss: 0.08148193359375\n",
      "F1 Train Score: 0.9886105060577393  | F1 Test Score: 0.9889108538627625\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.08148193359375 |BCEWithLogitsLoss Test Loss: 0.080078125\n",
      "F1 Train Score: 0.9889108538627625  | F1 Test Score: 0.9889108538627625\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.080078125 |BCEWithLogitsLoss Test Loss: 0.078857421875\n",
      "F1 Train Score: 0.9889108538627625  | F1 Test Score: 0.9895120859146118\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.078857421875 |BCEWithLogitsLoss Test Loss: 0.07757568359375\n",
      "F1 Train Score: 0.9895120859146118  | F1 Test Score: 0.9904153347015381\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.07757568359375 |BCEWithLogitsLoss Test Loss: 0.07635498046875\n",
      "F1 Train Score: 0.9904153347015381  | F1 Test Score: 0.990108072757721\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.07635498046875 |BCEWithLogitsLoss Test Loss: 0.0753173828125\n",
      "F1 Train Score: 0.990108072757721  | F1 Test Score: 0.9902587532997131\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.0753173828125 |BCEWithLogitsLoss Test Loss: 0.07415771484375\n",
      "F1 Train Score: 0.9902587532997131  | F1 Test Score: 0.9905602931976318\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.07415771484375 |BCEWithLogitsLoss Test Loss: 0.07318115234375\n",
      "F1 Train Score: 0.9905602931976318  | F1 Test Score: 0.9905602931976318\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.07318115234375 |BCEWithLogitsLoss Test Loss: 0.07208251953125\n",
      "F1 Train Score: 0.9905602931976318  | F1 Test Score: 0.9905602931976318\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.07208251953125 |BCEWithLogitsLoss Test Loss: 0.07098388671875\n",
      "F1 Train Score: 0.9905602931976318  | F1 Test Score: 0.9916171431541443\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.07098388671875 |BCEWithLogitsLoss Test Loss: 0.070068359375\n",
      "F1 Train Score: 0.9916171431541443  | F1 Test Score: 0.9919195175170898\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.070068359375 |BCEWithLogitsLoss Test Loss: 0.069091796875\n",
      "F1 Train Score: 0.9919195175170898  | F1 Test Score: 0.9922220706939697\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.069091796875 |BCEWithLogitsLoss Test Loss: 0.068115234375\n",
      "F1 Train Score: 0.9922220706939697  | F1 Test Score: 0.9922220706939697\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.068115234375 |BCEWithLogitsLoss Test Loss: 0.06732177734375\n",
      "F1 Train Score: 0.9922220706939697  | F1 Test Score: 0.9928277134895325\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.06732177734375 |BCEWithLogitsLoss Test Loss: 0.06622314453125\n",
      "F1 Train Score: 0.9928277134895325  | F1 Test Score: 0.9926739931106567\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.06622314453125 |BCEWithLogitsLoss Test Loss: 0.06549072265625\n",
      "F1 Train Score: 0.9926739931106567  | F1 Test Score: 0.9929770827293396\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.06549072265625 |BCEWithLogitsLoss Test Loss: 0.06451416015625\n",
      "F1 Train Score: 0.9929770827293396  | F1 Test Score: 0.9929770827293396\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.06451416015625 |BCEWithLogitsLoss Test Loss: 0.0638427734375\n",
      "F1 Train Score: 0.9929770827293396  | F1 Test Score: 0.9929770827293396\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.0638427734375 |BCEWithLogitsLoss Test Loss: 0.0628662109375\n",
      "F1 Train Score: 0.9929770827293396  | F1 Test Score: 0.9929770827293396\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.0628662109375 |BCEWithLogitsLoss Test Loss: 0.062255859375\n",
      "F1 Train Score: 0.9929770827293396  | F1 Test Score: 0.9929770827293396\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.062255859375 |BCEWithLogitsLoss Test Loss: 0.061553955078125\n",
      "F1 Train Score: 0.9929770827293396  | F1 Test Score: 0.9929770827293396\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.061553955078125 |BCEWithLogitsLoss Test Loss: 0.060760498046875\n",
      "F1 Train Score: 0.9929770827293396  | F1 Test Score: 0.9928211569786072\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.060760498046875 |BCEWithLogitsLoss Test Loss: 0.0601806640625\n",
      "F1 Train Score: 0.9928211569786072  | F1 Test Score: 0.9928211569786072\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.0601806640625 |BCEWithLogitsLoss Test Loss: 0.05950927734375\n",
      "F1 Train Score: 0.9928211569786072  | F1 Test Score: 0.9928211569786072\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.05950927734375 |BCEWithLogitsLoss Test Loss: 0.058685302734375\n",
      "F1 Train Score: 0.9928211569786072  | F1 Test Score: 0.9926672577857971\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.058685302734375 |BCEWithLogitsLoss Test Loss: 0.05804443359375\n",
      "F1 Train Score: 0.9926672577857971  | F1 Test Score: 0.9926672577857971\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.05804443359375 |BCEWithLogitsLoss Test Loss: 0.0574951171875\n",
      "F1 Train Score: 0.9926672577857971  | F1 Test Score: 0.9926672577857971\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.0574951171875 |BCEWithLogitsLoss Test Loss: 0.057037353515625\n",
      "F1 Train Score: 0.9926672577857971  | F1 Test Score: 0.993122398853302\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.057037353515625 |BCEWithLogitsLoss Test Loss: 0.05615234375\n",
      "F1 Train Score: 0.993122398853302  | F1 Test Score: 0.993122398853302\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.05615234375 |BCEWithLogitsLoss Test Loss: 0.055694580078125\n",
      "F1 Train Score: 0.993122398853302  | F1 Test Score: 0.993122398853302\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "BCEWithLogitsLoss Train Loss: 0.055694580078125 |BCEWithLogitsLoss Test Loss: 0.05517578125\n",
      "F1 Train Score: 0.993122398853302  | F1 Test Score: 0.993122398853302\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "linear_cl.train_model(input= X_train.to(device), labels=y_train.to(device), \n",
    "                      loss_fn=loss_fn, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An F1 of +99%, with such a simple model seems too good to be true...\n",
    "There could be some sort of label data leakage in the dataset, specially if we look at the test and train F1 score and notice that they are equal. This is highly suspicious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Score Brackets\n",
    "## A MultiClass Problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset can be found [here](https://www.kaggle.com/datasets/sudhanshu2198/processed-data-credit-score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
